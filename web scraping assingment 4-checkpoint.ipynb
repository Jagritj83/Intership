{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "176d07d5",
   "metadata": {},
   "source": [
    "# web scraping assignment-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae75be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotVisibleException,ElementClickInterceptedException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import requests\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e524ca",
   "metadata": {},
   "source": [
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url \n",
    "= https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details: A)\n",
    "Rank \n",
    "B) Name \n",
    "C) Artist \n",
    "D) Upload date \n",
    "E) Views "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e391d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos'\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(3)\n",
    "rank=[]\n",
    "name=[]\n",
    "artist=[]\n",
    "upload_date=[]\n",
    "views=[]\n",
    "\n",
    "rank_tag=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]/tbody/tr/td[1]')\n",
    "try:\n",
    "    for i in rank_tag[0:30]:\n",
    "        rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    rank.append(\"-\")\n",
    "except ElementNotVisibleException:\n",
    "    rank.append(\"--\")\n",
    "    \n",
    "name_tag=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]/tbody/tr/td[2]')\n",
    "try:\n",
    "    for i in name_tag[0:30]:\n",
    "        v_name=i.text.split('[')\n",
    "        name.append(v_name[0])\n",
    "except NoSuchElementException:\n",
    "    name.append(\"-\")\n",
    "except ElementNotVisibleException:\n",
    "    name.append(\"--\")\n",
    "    artist_tag=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]/tbody/tr/td[3]')\n",
    "try:\n",
    "    for i in artist_tag[0:30]:\n",
    "        artist.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    artist.append(\"-\")\n",
    "    \n",
    "date_tag=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]/tbody/tr/td[5]')\n",
    "try:\n",
    "    for i in date_tag[0:30]:\n",
    "        upload_date.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    upload_date.append(\"-\")\n",
    "    \n",
    "view_tag=driver.find_elements(By.XPATH,'//table[@class=\"wikitable sortable jquery-tablesorter\"]/tbody/tr/td[4]')\n",
    "try:\n",
    "    for i in view_tag[0:30]:\n",
    "        views.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    views.append(\"-\")\n",
    "    \n",
    "driver.close()\n",
    "\n",
    "most_viewed=pd.DataFrame({'Rank':rank,'Video Name':name,'Artist':artist,'Upload Date':upload_date,'Views':views})\n",
    "most_viewed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3d9565",
   "metadata": {},
   "source": [
    "2. Scrape the details team Indiaâ€™s international fixtures from bcci.tv. \n",
    "Url = https://www.bcci.tv/. \n",
    "You need to find following details: \n",
    "A) Series \n",
    "B) Place \n",
    "C) Date \n",
    "D) Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f66c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.bcci.tv'\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "driver.get(url)\n",
    "\n",
    "match_title=[]\n",
    "series=[]\n",
    "place=[]\n",
    "date=[]\n",
    "m_time=[]\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "#Click on INTERNATIONAL navigation\n",
    "driver.find_element(By.XPATH,\"//a[normalize-space()='INTERNATIONAL']\").click()\n",
    "\n",
    "time.sleep(3)\n",
    "for _ in range(1):\n",
    "    driver.execute_script(\"window.scrollBy(0,1000)\")\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        driver.find_element(By.XPATH,'/html[1]/body[1]/div[2]/div[2]/div[1]/div[1]/div[1]/div[2]/div[3]/div[2]/div[1]/button[1]').click()\n",
    "    except NoSuchElementException:\n",
    "        print(\"NoSuchElementException \")\n",
    "    \n",
    "title_tag=driver.find_elements(By.XPATH,'//span[@class=\"ng-binding\"]')\n",
    "try:\n",
    "    for i in title_tag:\n",
    "        match_title.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    match_title.append(\"-\")\n",
    "match_title\n",
    "series_tag=driver.find_elements(By.XPATH,'//div[@class=\"fix-place ng-binding ng-scope\"]')\n",
    "try:\n",
    "    for i in series_tag:\n",
    "        s_tag=i.text.split('-')\n",
    "        series.append(s_tag[0])\n",
    "except NoSuchElementException:\n",
    "    series.append(\"-\")\n",
    "    \n",
    "\n",
    "place_tag=driver.find_elements(By.XPATH,'//div[@class=\"fix-place ng-binding ng-scope\"]')\n",
    "try:\n",
    "    for i in place_tag:\n",
    "        p_tag=i.text.split('-')\n",
    "        place.append(p_tag[1])\n",
    "except NoSuchElementException:\n",
    "    place.append(\"-\")\n",
    "    \n",
    "\n",
    "\n",
    "date_tag=driver.find_elements(By.XPATH,'//h5[@class=\"ng-binding\"]')\n",
    "try:\n",
    "    for i in date_tag:\n",
    "        date.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    date.append(\"-\")\n",
    "    \n",
    "\n",
    "time_tag=driver.find_elements(By.XPATH,'//h5[@class=\"text-right ng-binding\"]')\n",
    "try:\n",
    "    for i in time_tag:\n",
    "        m_time.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    m_time.append(\"-\")\n",
    "    \n",
    "driver.close()\n",
    "international_fixtures=pd.DataFrame({'Match Title':match_title,'Series':series,'Place':place,'Date':date,'Time':m_time})\n",
    "international_fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1e5004",
   "metadata": {},
   "source": [
    "3. Scrape the details of State-wise GDP of India from statisticstime.com. \n",
    "Url = http://statisticstimes.com/ \n",
    "You have to find following details: A) Rank \n",
    "B) State \n",
    "C) GSDP(18-19)- at current prices \n",
    "D) GSDP(19-20)- at current prices \n",
    "E) Share(18-19) \n",
    "F) GDP($ billion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29de7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.statisticstimes.com'\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(3)\n",
    "economy = driver.find_element(By.XPATH,\"//*[@id='top']/div[2]/div[2]/button\")\n",
    "try:\n",
    "    economy.click()\n",
    "    driver.find_element(By.XPATH,'/html[1]/body[1]/div[2]/div[1]/div[2]/div[2]/div[1]/a[3]').click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(economy.get_attribute('href'))\n",
    "    \n",
    "time.sleep(3)\n",
    "try:\n",
    "    ads=driver.find_element(By.XPATH,'//div[@class=\"ns-dbyfr-e-19 button-common close-button\"]')\n",
    "    ads.click()\n",
    "    gdp = driver.find_element(By.XPATH,\"/html/body/div[2]/div[2]/div[2]/ul/li[1]/a\").click()\n",
    "except NoSuchElementException:\n",
    "    driver.get('https://www.statisticstimes.com/economy/india/indian-states-gdp.php')\n",
    "    \n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "rank=[]\n",
    "state=[]\n",
    "GSDP_19_20=[]\n",
    "GSDP_18_19=[]\n",
    "share_18_19=[]\n",
    "GDP_billion=[]\n",
    "state_rank=driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[1]')\n",
    "try:\n",
    "    for i in state_rank:\n",
    "        rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    rank.append(\"-\")\n",
    "    \n",
    "state_name=driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[2]')\n",
    "try:\n",
    "    for i in state_name:\n",
    "        state.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    state.append(\"-\")\n",
    "        \n",
    "GSDP_19_20_tag=driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[3]')\n",
    "try:\n",
    "    for i in GSDP_19_20_tag:\n",
    "        GSDP_19_20.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP_19_20.append(\"-\")\n",
    "    \n",
    "GSDP_18_19_tag=driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[4]')\n",
    "try:\n",
    "    for i in GSDP_18_19_tag:\n",
    "        GSDP_18_19.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP_18_19.append(\"-\")\n",
    "    \n",
    "share_18_19_tag=driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[5]')\n",
    "try:\n",
    "    for i in share_18_19_tag:\n",
    "        share_18_19.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    share_18_19.append(\"-\")\n",
    "    GDP_billion_tag=driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[6]')\n",
    "try:\n",
    "    for i in GDP_billion_tag:\n",
    "        GDP_billion.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GDP_billion.append(\"-\")\n",
    "    \n",
    "    \n",
    "driver.close()\n",
    "\n",
    "StateWise_GDP=pd.DataFrame({'Rank':rank,'State':state,'GSDP 19-20':GSDP_19_20,'GSDP 18-19':GSDP_18_19,'Share 18-19':share_18_19,'GDP billion':GDP_billion})\n",
    "StateWise_GDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1a057",
   "metadata": {},
   "source": [
    "4. Scrape the details of trending repositories on Github.com. \n",
    "Url = https://github.com/ \n",
    "You have to find the following details: \n",
    "A) Repository title \n",
    "B) Repository description \n",
    "C) Contributors count \n",
    "D) Language used \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3f502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "driver.maximize_window()\n",
    "url=\"https://github.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "# Clicking on explore sub menu\n",
    "explore = driver.find_element(By.XPATH,'/html[1]/body[1]/div[1]/header[1]/div[1]/div[2]/div[1]/nav[1]/ul[1]/li[3]/button[1]')\n",
    "try:\n",
    "    explore.click()\n",
    "    time.sleep(5)\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(explore.get_attribute('href'))\n",
    "\n",
    "    \n",
    "# Clicking on Trending under explore sub menu\n",
    "trending = driver.find_element(By.XPATH,'//*[@href=\"/trending\"]')\n",
    "try:\n",
    "    driver.get(trending.get_attribute('href'))\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(trending.get_attribute('href'))\n",
    "    \n",
    "Repository_title= []\n",
    "Repository_description= []\n",
    "Contributors_count= []\n",
    "Language_used= []\n",
    "urls=[]\n",
    "\n",
    "#Scraping title of repository\n",
    "try:\n",
    "    Repository_title_tag=driver.find_elements(By.XPATH,'//article[@class=\"Box-row\"]/h1/a')\n",
    "    for i in Repository_title_tag:\n",
    "        Repository_title.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Repository_title.append('-')\n",
    "    #Scraping description of Repository\n",
    "try:\n",
    "    description=driver.find_elements(By.XPATH,'//article[@class=\"Box-row\"]/p')\n",
    "    for i in description:\n",
    "        Repository_description.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Repository_description.append('-')\n",
    "\n",
    "for i in [Repository_title_tag,description]:\n",
    "    for j in i:\n",
    "        if i ==Repository_title_tag:\n",
    "            Repository_title.append(j.text)\n",
    "            urls.append(j.get_attribute('href'))\n",
    "        if i==description:\n",
    "            Repository_description.append(j.text)\n",
    "    \n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(3)\n",
    "    div_list=driver.find_elements(By.XPATH,'//div[@class=\"BorderGrid BorderGrid--spacious\"]/div')\n",
    "    #Scraping count of contributors   \n",
    "    try:\n",
    "        Contributors_count.append(((div_list[-2].text).split())[1])\n",
    "    except:\n",
    "        Contributors_count.append('-')\n",
    "    #Scraping used language\n",
    "    try:\n",
    "        Language_used.append(((div_list[-1].text).split())[1::2])\n",
    "    except:\n",
    "        Language_used.append('-')\n",
    "        \n",
    "driver.close()\n",
    "\n",
    "\n",
    "#creating Dataframe\n",
    "github_trending=pd.DataFrame({\"Repository_title\":Repository_title[:25],\"Repository_description\":Repository_description[:25],\"Language_used\":Language_used[:25],\"Contributors_count\":Contributors_count[:25]})\n",
    "github_trending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69dc1bb",
   "metadata": {},
   "source": [
    "5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find the \n",
    "following details: \n",
    "A) Song name \n",
    "B) Artist name \n",
    "C) Last week rank \n",
    "D) Peak rank \n",
    "E) Weeks on board "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d08ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "driver.maximize_window()\n",
    "url='https://www.billboard.com'\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "nav=driver.find_element(By.XPATH,\"/html/body/div[3]/header/div[1]/div/div/div[1]/button\")\n",
    "nav.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "#Hot 100 songs\n",
    "hot_100=driver.find_element(By.XPATH,'/html[1]/body[1]/div[3]/div[6]/div[1]/div[1]/div[1]/ul[1]/li[1]/ul[1]/li[2]/a[1]')\n",
    "hot_100.click()\n",
    "\n",
    "Name=[]\n",
    "Artist=[]\n",
    "rank=[]\n",
    "\n",
    "#Scraping name\n",
    "Name_tag=driver.find_elements(By.XPATH,\"//h3[@class='c-title  a-no-trucate a-font-primary-bold-s u-letter-spacing-0021 u-font-size-23@tablet lrv-u-font-size-16 u-line-height-125 u-line-height-normal@mobile-max a-truncate-ellipsis u-max-width-245 u-max-width-230@tablet-only u-letter-spacing-0028@tablet']\")\n",
    "for i in Name_tag:\n",
    "    Name.append(i.text)\n",
    "\n",
    "#Scraping artist\n",
    "Artist_tag=driver.find_elements(By.XPATH,\"//span[@class='c-label  a-no-trucate a-font-primary-s lrv-u-font-size-14@mobile-max u-line-height-normal@mobile-max u-letter-spacing-0021 lrv-u-display-block a-truncate-ellipsis-2line u-max-width-330 u-max-width-230@tablet-only u-font-size-20@tablet']\")\n",
    "for i in Artist_tag:\n",
    "    Artist.append(i.text)\n",
    "\n",
    "#Scraping rank\n",
    "rankTag=driver.find_elements(By.XPATH,\"//span[@class='c-label  a-font-primary-bold-l a-font-primary-m@mobile-max u-font-weight-normal@mobile-max lrv-u-padding-tb-050@mobile-max u-font-size-32@tablet']\")\n",
    "for i in rankTag[:3]:\n",
    "    rank.append(i.text )\n",
    "    #Scraping name\n",
    "Rank=[]\n",
    "nameTag=driver.find_elements(By.XPATH,\"//h3[@class='c-title  a-no-trucate a-font-primary-bold-s u-letter-spacing-0021 lrv-u-font-size-18@tablet lrv-u-font-size-16 u-line-height-125 u-line-height-normal@mobile-max a-truncate-ellipsis u-max-width-330 u-max-width-230@tablet-only']\")\n",
    "for i in nameTag:\n",
    "    Name.append(i.text )\n",
    "\n",
    "#Scraping artist\n",
    "artistTag=driver.find_elements(By.XPATH,\"//span[@class='c-label  a-no-trucate a-font-primary-s lrv-u-font-size-14@mobile-max u-line-height-normal@mobile-max u-letter-spacing-0021 lrv-u-display-block a-truncate-ellipsis-2line u-max-width-330 u-max-width-230@tablet-only']\")\n",
    "for i in artistTag:\n",
    "    Artist.append(i.text )\n",
    "\n",
    "#Scraping rank\n",
    "RankTag=driver.find_elements(By.XPATH,\"//span[@class='c-label  a-font-primary-m lrv-u-padding-tb-050@mobile-max']\")\n",
    "for i in RankTag:\n",
    "    Rank.append(i.text)\n",
    "    \n",
    "    \n",
    "# removing ''\n",
    "for i in Rank:\n",
    "    if i=='':\n",
    "        Rank.remove(i)\n",
    "        \n",
    "# slicing Data\n",
    "last_week_rank=Rank[0::3]\n",
    "last_week_rank.insert(0,rank[0])\n",
    "peak_rank=Rank[1::3]\n",
    "peak_rank.insert(0,rank[1])\n",
    "weeks_on_board=Rank[2::3]\n",
    "weeks_on_board.insert(0,rank[2])\n",
    "\n",
    "driver.close()\n",
    "# Create Dataframe\n",
    "songs_top=pd.DataFrame({\"Name\":Name[:100],\"Artist\":Artist[:100],\"Last Week Rank\":last_week_rank[0:100],\"Peak Rank\":peak_rank[0:100],\"Weeks On Board\":weeks_on_board[0:100]})\n",
    "songs_top"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c516fd2",
   "metadata": {},
   "source": [
    "6. Scrape the details of Highest selling novels. \n",
    "A) Book name \n",
    "B) Author name \n",
    "C) Volumes sold \n",
    "D) Publisher \n",
    "E) Genre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e763fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "driver.maximize_window()\n",
    "url='https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare'\n",
    "driver.get(url)\n",
    "\n",
    "book_name=[]\n",
    "author_name=[]\n",
    "volumes_sold=[]\n",
    "publisher=[]\n",
    "genre=[]\n",
    "\n",
    "b_name=driver.find_elements(By.XPATH,'/html[1]/body[1]/div[1]/div[2]/div[2]/div[1]/div[2]/div[1]/table[1]/tbody[1]/tr/td[2]')\n",
    "try:\n",
    "    for i in b_name:\n",
    "        book_name.append(i.text)\n",
    "except:\n",
    "    book_name.append(\"-\")\n",
    "    \n",
    "a_name=driver.find_elements(By.XPATH,'/html[1]/body[1]/div[1]/div[2]/div[2]/div[1]/div[2]/div[1]/table[1]/tbody[1]/tr/td[3]')\n",
    "try:\n",
    "    for i in a_name:\n",
    "        author_name.append(i.text)\n",
    "except:\n",
    "    author_name.append(\"-\")\n",
    "    \n",
    "v_sold=driver.find_elements(By.XPATH,'/html[1]/body[1]/div[1]/div[2]/div[2]/div[1]/div[2]/div[1]/table[1]/tbody[1]/tr/td[4]')\n",
    "try:\n",
    "    for i in v_sold:\n",
    "        volumes_sold.append(i.text)\n",
    "except:\n",
    "    volumes_sold.append(\"-\")\n",
    "    \n",
    "b_publisher=driver.find_elements(By.XPATH,'/html[1]/body[1]/div[1]/div[2]/div[2]/div[1]/div[2]/div[1]/table[1]/tbody[1]/tr/td[5]')\n",
    "try:\n",
    "    for i in b_publisher:\n",
    "        publisher.append(i.text)\n",
    "except:\n",
    "    publisher.append(\"-\")\n",
    "    b_genre=driver.find_elements(By.XPATH,'/html[1]/body[1]/div[1]/div[2]/div[2]/div[1]/div[2]/div[1]/table[1]/tbody[1]/tr/td[6]')\n",
    "try:\n",
    "    for i in b_genre:\n",
    "        genre.append(i.text)\n",
    "except:\n",
    "    genre.append(\"-\")\n",
    "    \n",
    "driver.close()\n",
    "Highest_selling_novels=pd.DataFrame({'Book Name':book_name,'Author Name':author_name,'Volumes sold':volumes_sold,'Publisher':publisher,'Genre':genre})\n",
    "Highest_selling_novels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e3a244",
   "metadata": {},
   "source": [
    "7. Scrape the details most watched tv series of all time from imdb.com. \n",
    "Url = https://www.imdb.com/list/ls095964455/ You have \n",
    "to find the following details: \n",
    "A) Name \n",
    "B) Year span \n",
    "C) Genre \n",
    "D) Run time \n",
    "E) Ratings \n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67156e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "driver.maximize_window()\n",
    "url='https://www.imdb.com/list/ls095964455'\n",
    "driver.get(url)\n",
    "\n",
    "name=[]\n",
    "year=[]\n",
    "genre=[]\n",
    "run_time=[]\n",
    "ratings=[]\n",
    "votes=[]\n",
    "\n",
    "s_name=driver.find_elements(By.XPATH,'//h3[@class=\"lister-item-header\"]/a')\n",
    "try:\n",
    "    for i in s_name:\n",
    "        name.append(i.text)\n",
    "except:\n",
    "    name.append(\"--\") \n",
    "    \n",
    "s_year=driver.find_elements(By.XPATH,'//h3[@class=\"lister-item-header\"]/span[2]')\n",
    "try:\n",
    "    for i in s_year:\n",
    "        year.append(i.text)\n",
    "except:\n",
    "    year.append(\"--\")   \n",
    "\n",
    "s_genre=driver.find_elements(By.XPATH,'//p[@class=\"text-muted text-small\"]/span[5]')\n",
    "try:\n",
    "    for i in s_genre:\n",
    "        genre.append(i.text)\n",
    "except:\n",
    "    genre.append(\"--\")\n",
    "    \n",
    "r_time=driver.find_elements(By.XPATH,'//p[@class=\"text-muted text-small\"]/span[3]')\n",
    "try:\n",
    "    for i in r_time:\n",
    "        run_time.append(i.text)\n",
    "        except:\n",
    "    run_time.append(\"--\")   \n",
    "\n",
    "rating=driver.find_elements(By.XPATH,'//div[@class=\"ipl-rating-widget\"]/div/span[2]')\n",
    "try:\n",
    "    for i in rating:\n",
    "        ratings.append(i.text)\n",
    "except:\n",
    "    ratings.append(\"--\")\n",
    "    \n",
    "vote=driver.find_elements(By.XPATH,'//span[@name=\"nv\"]')\n",
    "try:\n",
    "    for i in vote:\n",
    "        votes.append(i.text)\n",
    "except:\n",
    "    votes.append(\"--\")\n",
    "    \n",
    "driver.close()\n",
    "tv_series=pd.DataFrame({'Name':name,'Year Span':year,'Genre':genre,'Run Time':run_time,'Ratings':ratings,'Votes':votes})\n",
    "tv_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4065de",
   "metadata": {},
   "source": [
    "8. Details of Datasets from UCI machine learning repositories. \n",
    "Url = https://archive.ics.uci.edu/ You \n",
    "have to find the following details: \n",
    "A) Dataset name \n",
    "B) Data type \n",
    "C) Task \n",
    "D) Attribute type \n",
    "E) No of instances \n",
    "F) No of attribute G) Year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0359983",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "driver.maximize_window()\n",
    "url='https://archive.ics.uci.edu/ml/index.php'\n",
    "driver.get(url)\n",
    "\n",
    "#clicking on view all Dataset\n",
    "Dataset= driver.find_elements(By.XPATH,\"//span[@class='normal']/b/a\")[0].get_attribute('href')\n",
    "driver.get(Dataset)\n",
    "\n",
    "# Scraping dataset\n",
    "data=[]\n",
    "try:\n",
    "    data_tag=driver.find_elements(By.XPATH,\"//td//p[@class='normal']\")\n",
    "    for i in data_tag[8:4362]:\n",
    "        data.append(i.text )\n",
    "except:\n",
    "    data.append('-')\n",
    "    \n",
    "#slicing dataset\n",
    "Dataset_name=data[::7]\n",
    "Data_type=data[1::7]\n",
    "Task=data[2::7]\n",
    "Attribute_type=data[3::7]\n",
    "instances=data[4::7]\n",
    "attribute=data[5::7]\n",
    "Year=data[6::7]\n",
    "\n",
    "driver.close()\n",
    "#creating Dataframe\n",
    "Datasets= pd.DataFrame({\"Dataset name\":Dataset_name,\"Data type\":Data_type,\"Task\":Task,\"Attribute type\":Attribute_type,\"Instances\":instances,\"Attribute\":attribute,\"Year\":Year})\n",
    "Datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
