{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3810c416",
   "metadata": {},
   "source": [
    "# web scraping assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f47b13",
   "metadata": {},
   "source": [
    "# Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "358e0642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\jagri\\anaconda3\\lib\\site-packages (4.15.2)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\jagri\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\jagri\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\jagri\\anaconda3\\lib\\site-packages (from selenium) (0.23.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\jagri\\anaconda3\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\jagri\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\jagri\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\jagri\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: idna in c:\\users\\jagri\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\jagri\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\jagri\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\jagri\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\jagri\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\jagri\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\jagri\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\jagri\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76ff704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium \n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c455df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d7daa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.shine.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5379f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys(\"Data Analyst\")\n",
    "\n",
    "location=driver.find_element(By.XPATH,'//input[@placeholder=\"Enter location\"]')\n",
    "location.send_keys('Bangalore')\n",
    "\n",
    "search=driver.find_element(By.XPATH,'//div[@class=\"qsbSubmit\"]')\n",
    "search.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "job_company=[]\n",
    "job_experience=[]\n",
    "\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "\n",
    "location_tags=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "\n",
    "company_tags=driver.find_elements(By.XPATH,\"//a[@class='subTitle ellipsis fleft']\")\n",
    "for i in company_tags[0:10]:\n",
    "    cname=i.text\n",
    "    job_company.append(cname)\n",
    "    \n",
    "experience_tags=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span\")\n",
    "for i  in experience_tags[0:10]:\n",
    "    experience=i.text\n",
    "    job_experience.append(experience)\n",
    "\n",
    "driver.close()\n",
    "\n",
    "dataAnalyst=pd.DataFrame({'job-title ':job_title,'job-location':job_location,'company_name':job_company,'experience_required':job_experience})\n",
    "dataAnalyst   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104b4d4d",
   "metadata": {},
   "source": [
    "# Q2:Write a python program to scrape data for “Data Scientist” Job position in“Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Scientist” in “Job title, Skills” field and enter “Bangalore” in “enter thelocation” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c823cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"chromedriver.exe\")  \n",
    "driver.maximize_window()\n",
    "driver.get(\" https://www.shine.com/\")\n",
    "\n",
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys(\"Data Scientist\")\n",
    "\n",
    "location=driver.find_element(By.XPATH,'//input[@placeholder=\"Enter location\"]')\n",
    "location.send_keys('Bangalore')\n",
    "\n",
    "search=driver.find_element(By.XPATH,'//div[@class=\"qsbSubmit\"]')\n",
    "search.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "job_company=[]\n",
    "job_experience=[]\n",
    "\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "\n",
    "location_tags=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "\n",
    "company_tags=driver.find_elements(By.XPATH,\"//a[@class='subTitle ellipsis fleft']\")\n",
    "for i in company_tags[0:10]:\n",
    "    cname=i.text\n",
    "    job_company.append(cname)\n",
    "\n",
    "driver.close()\n",
    "\n",
    "dataScientist=pd.DataFrame({'job-title ':job_title,'job-location':job_location,'company_name':job_company})\n",
    "dataScientist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81995cf2",
   "metadata": {},
   "source": [
    "# : In this question you have to scrape data using the filters available on the webpage \n",
    " You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the web page https://www.shine.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scrapeddata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ce17b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"chromedriver.exe\")  \n",
    "driver.maximize_window()\n",
    "driver.get(\" https://www.shine.com/\")\n",
    "\n",
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input\")\n",
    "designation.send_keys(\"Data Scientist\")\n",
    "\n",
    "#location=driver.find_element(By.XPATH,'//input[@placeholder=\"Enter location\"]')\n",
    "#location.send_keys('Bangalore')\n",
    "search=driver.find_element(By.XPATH,'//div[@class=\"qsbSubmit\"]')\n",
    "search.click()\n",
    "time.sleep(3)\n",
    "driver.find_element(By.XPATH,\"//span[@title='Delhi / NCR']\").click()\n",
    "time.sleep(3)\n",
    "driver.find_element(By.XPATH,\"//span[@title='0-3 Lakhs']\").click()\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "job_company=[]\n",
    "job_experience=[]\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "\n",
    "location_tags=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "company_tags=driver.find_elements(By.XPATH,\"//a[@class='subTitle ellipsis fleft']\")\n",
    "for i in company_tags[0:10]:\n",
    "    cname=i.text\n",
    "    job_company.append(cname)\n",
    "\n",
    "\n",
    "experience_tags=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span\")\n",
    "for i  in experience_tags[0:10]:\n",
    "    experience=i.text\n",
    "    job_experience.append(experience)\n",
    "\n",
    "driver.close()\n",
    "\n",
    "data_ScientistLoc=pd.DataFrame({'job-title ':job_title,'job-location':job_location,'company_name':job_company,'experience_required':job_experience})\n",
    "data_ScientistLoc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5213f759",
   "metadata": {},
   "source": [
    "# Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "6. Brand\n",
    "7. ProductDescription\n",
    "8. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8f166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"chromedriver.exe\")  \n",
    "driver.maximize_window()\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "try:\n",
    "    driver.find_element(By.XPATH,'//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "except NoSuchElementException:\n",
    "    print('No login pop-up')\n",
    "    \n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "driver.find_element(By.XPATH,'//input[@class=\"_3704LK\"]').send_keys('sunglasses')\n",
    "driver.find_element(By.XPATH,'//button[@class=\"L0Z3Pu\"]').click()\n",
    "\n",
    "brand=[]\n",
    "product_description=[]\n",
    "price=[]\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "for page in range(0,3):\n",
    "    brand_tags=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    for i in brand_tags[0:100]:\n",
    "        brandName=i.text\n",
    "        brand.append(brandName)\n",
    "    \n",
    "    product_tags=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    for j in product_tags[0:100]:\n",
    "        productD=j.text\n",
    "        product_description.append(productD)\n",
    "     price_tags=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "    for k in price_tags[0:100]:\n",
    "        price_t=k.text\n",
    "        price.append(price_t)\n",
    "    \n",
    "time.sleep(3)\n",
    "next_page=driver.find_element(By.XPATH,'//a[@class=\"ge-49M\"]')\n",
    "next_page.click\n",
    "    \n",
    "driver.close()\n",
    "\n",
    "sunglasses_df=pd.DataFrame({'Brand':brand[0:100],'Product Description':product_description[0:100],'Price':price[0:100]})\n",
    "sunglasses_df    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11979a7",
   "metadata": {},
   "source": [
    "# Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/p/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZE3ENS&marketplace=FLIPKART&q=iphone+11&store=tyy/4io&srno=s_1_1&otracker=AS_Query_HistoryAutoSuggest_6_0_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_6_0_na_na_na&fm=organic&iid=e30aaa70-fe6e-466c-81d4-993e630ea913.MOBFWQ6BXGJCEYNY.SEARCH&ppt=hp&ppn=homepage&ssid=nseg9l47e80000001699187524593&qH=f6cdfdaa9f3c23f3\n",
    "\n",
    "As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1717454",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"chromedriver.exe\")  \n",
    "driver.maximize_window()\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "try:\n",
    "    driver.find_element(By.XPATH,'//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "except NoSuchElementException:\n",
    "    print('No login pop-up')\n",
    "    \n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element(By.XPATH,'//input[@class=\"_3704LK\"]').send_keys('iphone11')\n",
    "driver.find_element(By.XPATH,'//button[@class=\"L0Z3Pu\"]').click()\n",
    "\n",
    "time.sleep(3)\n",
    "driver.find_element(By.XPATH,'//div[@class=\"_4rR01T\"]').click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "driver.switch_to.window(driver.window_handles[1])\n",
    "\n",
    "driver.find_element(By.XPATH,'//div[@class=\"_3UAT2v _16PBlm\"]/span').click()\n",
    "\n",
    "rating=[]\n",
    "review_summary=[]\n",
    "full_review=[]\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "start=0\n",
    "end=10\n",
    "\n",
    " for page in range(start,end):\n",
    "    \n",
    "    rating_tag=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    for i in rating_tag[0:100]:\n",
    "        rating_all=i.text\n",
    "        rating.append(rating_all)\n",
    "    \n",
    "    summary_rs=driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "    for i in summary_rs[0:100]:\n",
    "        summary_review=i.text\n",
    "        review_summary.append(summary_review)\n",
    "    \n",
    "    review_full=driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]/div/div')\n",
    "    for i in review_full[0:100]:\n",
    "        review_fullAll=i.text\n",
    "        full_review.append(review_fullAll)\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "next_page=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]/span')\n",
    "next_page.click\n",
    "time.sleep(3)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "review_in_deep=pd.DataFrame({'Rating':rating,'Review summary':review_summary,'Full review':full_review})\n",
    "review_in_deep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01638abc",
   "metadata": {},
   "source": [
    "# Q6: Scrape data forfirst 100 sneakers you find whenyou visit flipkart.com and search for “sneakers” inthe\n",
    "search field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbf13c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"chromedriver.exe\")  \n",
    "driver.maximize_window()\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "time.sleep(3)\n",
    "try:\n",
    "    driver.find_element(By.XPATH,'//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "except NoSuchElementException:\n",
    "    print('No login pop-up')\n",
    "    time.sleep(2)\n",
    "\n",
    "driver.find_element(By.XPATH,'//input[@class=\"_3704LK\"]').send_keys('sneakers')\n",
    "driver.find_element(By.XPATH,'//button[@class=\"L0Z3Pu\"]').click()\n",
    "\n",
    "time.sleep(3)\n",
    "sneakers_brand=[]\n",
    "sneakers_prodDesc=[]\n",
    "sneakers_price=[]\n",
    "sneaker_discount=[]\n",
    "time.sleep(3)\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    sBrand_tags=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    for i in sBrand_tags[0:100]:\n",
    "        s_brand=i.text\n",
    "        sneakers_brand.append(s_brand)\n",
    "    \n",
    "    s_prodD=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa _2-ICcC\"]')\n",
    "    s_prodD1=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    \n",
    "    for i in s_prodD1[0:100]:\n",
    "        sprod_d1=i.text\n",
    "        sneakers_prodDesc.append(sprod_d1)\n",
    "    \n",
    "    for i in s_prodD[0:100]:\n",
    "        sprod_d=i.text\n",
    "        sneakers_prodDesc.append(sprod_d)\n",
    "    \n",
    "    s_price=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "    for i in s_price[0:100]:\n",
    "        sprice=i.text\n",
    "        sneakers_price.append(sprice)\n",
    "        \n",
    "    sneaker_disc=driver.find_elements(By.XPATH,'//div[@class=\"_3Ay6Sb\"]/span')\n",
    "    for i in sneaker_disc[0:100]:\n",
    "        s_disc=i.text\n",
    "        sneaker_discount.append(s_disc)\n",
    "        \n",
    "time.sleep(3)\n",
    "next_page=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]/span')\n",
    "next_page.click\n",
    "time.sleep(3)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "sneakers_dataframe=pd.DataFrame({'Brand':sneakers_brand[0:100],'Product Description':sneakers_prodDesc[0:100],'Price':sneakers_price[0:100],'Discount':sneaker_discount[0:100]})\n",
    "sneakers_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832b7f44",
   "metadata": {},
   "source": [
    "# Q7: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then\n",
    "set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddbc686",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"chromedriver.exe\")  \n",
    "driver.maximize_window()\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "textBox=driver.find_element(By.ID,'twotabsearchtextbox')\n",
    "textBox.send_keys('Laptop')\n",
    "\n",
    "submit=driver.find_element(By.ID,'nav-search-submit-button')\n",
    "submit.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "cpu_type=driver.find_element(By.XPATH,'//*[@id=\"p_n_feature_thirteen_browse-bin/12598163031\"]/span/a/span')\n",
    "cpu_type.click()\n",
    "\n",
    "time.sleep(3)\n",
    "product_title=[]\n",
    "product_rating=[]\n",
    "product_price=[]\n",
    "\n",
    "title_tags=driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    product_title.append(i.text)\n",
    "    \n",
    "product_tags=driver.find_elements(By.XPATH,'//div[@class=\"a-row a-size-small\"]/span[1]')\n",
    "for i in product_tags[0:10]:\n",
    "    rating=i.get_attribute('aria-label')\n",
    "    product_rating.append(rating)\n",
    "    \n",
    "price_tags=driver.find_elements(By.XPATH,'//span[@class=\"a-price\"]')\n",
    "for i in price_tags[0:10]:\n",
    "    product_price.append(i.text)\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "laptop_dataFrame=pd.DataFrame({'Title':product_title,'Rating':product_rating,'Price':product_price})\n",
    "laptop_dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9881e09",
   "metadata": {},
   "source": [
    "# Q8: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1. First get the webpagehttps://www.azquotes.com/\n",
    "2. Click on TopQuotes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a495f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.azquotes.com/\")\n",
    "\n",
    "top_quotes_button = driver.find_element(By.LINK_TEXT, \"Top Quotes\")\n",
    "top_quotes_button.click()\n",
    "\n",
    "quotes = driver.find_elements(By.CSS_SELECTOR, \".title a\")\n",
    "authors = driver.find_elements(By.CSS_SELECTOR, \".author a\")\n",
    "types = driver.find_elements(By.CSS_SELECTOR, \".kw-box a\")\n",
    "\n",
    "for quote, author, quote_type in zip(quotes, authors, types):\n",
    "print(\"Quote:\", quote.text)\n",
    "print(\"Author:\", author.text)\n",
    "print(\"Type of Quote:\", quote_type.text)\n",
    "print()\n",
    "\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
